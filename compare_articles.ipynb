{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compare_articles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gablwjKV8u7Z",
        "outputId": "d2c4a19f-9164-40c2-f024-0832910feab9"
      },
      "source": [
        "%cd /content/drive/MyDrive/tl_assess"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/tl_assess'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VFjPjP681iL",
        "outputId": "39d043a1-0d48-4e97-c4e3-f97fb1780ad8"
      },
      "source": [
        "%set_env PYTHONPATH=$PYTHONPATH:/content/drive/MyDrive/tl_assess\n",
        "!echo $PYTHONPATH"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=$PYTHONPATH:/content/drive/MyDrive/tl_assess\n",
            "$PYTHONPATH:/content/drive/MyDrive/tl_assess\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEXBt3Ls82qX",
        "outputId": "a325776d-e707-4055-b8cb-fdc5c5751c30"
      },
      "source": [
        "!pip install textblob warcio sentence-transformers langdetect nltk"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: warcio in /usr/local/lib/python3.6/dist-packages (1.7.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.3.9)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from warcio) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: transformers<3.6.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.17.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.12.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (20.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers<3.6.0,>=3.1.0->sentence-transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiI1uwqG84HL",
        "outputId": "508d5956-dc1e-4903-9909-80a28acf9a59"
      },
      "source": [
        "import re\n",
        "import io\n",
        "import sys\n",
        "import gzip\n",
        "import string\n",
        "import pickle\n",
        "import requests\n",
        "import traceback\n",
        "import langdetect\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from langdetect import detect\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from warcio.archiveiterator import ArchiveIterator\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxPMmiap865Z"
      },
      "source": [
        "porter = PorterStemmer()\n",
        "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "sw = stopwords.words(\"english\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = regex.sub('', text)\n",
        "    text = ' '.join([word for word in text.split() if (len(word)>=4)])\n",
        "    text = ' '.join([word for word in text.split() if word not in sw])\n",
        "    text = ' '.join([porter.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "def get_page_content(html_page):\n",
        "    if html_page.startswith('https') or html_page.startswith('http'):\n",
        "        html_page = requests.get(html_page).content\n",
        "    soup = BeautifulSoup(html_page, 'html.parser')\n",
        "    return clean_text(soup.text)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH67RMCI89RO"
      },
      "source": [
        "ref_docs = [\n",
        "    get_page_content('https://en.wikipedia.org/wiki/Economic_impact_of_the_COVID-19_pandemic'),\n",
        "    get_page_content('https://www.who.int/news/item/13-10-2020-impact-of-covid-19-on-people\\'s-livelihoods-their-health-and-our-food-systems'),\n",
        "    get_page_content('https://www.brookings.edu/research/ten-facts-about-covid-19-and-the-u-s-economy/'),\n",
        "    get_page_content('https://www.mckinsey.com/business-functions/risk/our-insights/covid-19-implications-for-business'),\n",
        "    get_page_content('https://carsey.unh.edu/COVID-19-Economic-Impact-By-State'),\n",
        "    get_page_content('https://www.frontiersin.org/articles/10.3389/fpubh.2020.00241/full'),\n",
        "    get_page_content('https://www.pewsocialtrends.org/2020/09/24/economic-fallout-from-covid-19-continues-to-hit-lower-income-americans-the-hardest/'),\n",
        "    get_page_content('https://www.reuters.com/article/us-usa-economy-poll/u-s-economy-to-slow-in-first-quarter-but-reach-pre-covid-19-levels-in-a-year-reuters-poll-idUSKBN28K00A'),\n",
        "    get_page_content('https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/the-coronavirus-effect-on-global-economic-sentiment')\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "docs_tfidf = vectorizer.fit_transform(ref_docs)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra8nowSY9RCV"
      },
      "source": [
        "url_regex = re.compile(\n",
        "        r'^(?:http|ftp)s?://' # http:// or https://\n",
        "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
        "        r'localhost|' #localhost...\n",
        "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
        "        r'(?::\\d+)?' # optional port\n",
        "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
        "\n",
        "def get_content_from_warc(url):\n",
        "    resp = requests.get(url, stream=True)\n",
        "\n",
        "    for record in ArchiveIterator(resp.raw, arc2warc=True):\n",
        "        if record.rec_type == 'warcinfo':\n",
        "            continue\n",
        "        \n",
        "        if re.match(url_regex, record.rec_headers.get_header(\"WARC-Target-URI\")) is None:\n",
        "            continue\n",
        "\n",
        "        elif record.rec_type is not None and record.rec_type == 'response':\n",
        "            if record.http_headers is not None and  record.http_headers.get_header('Content-Type') is not None and record.http_headers.get_header('Content-Type') == 'text/html':\n",
        "                html_content = record.content_stream().read().decode(\"utf-8\", \"replace\")\n",
        "                if html_content is not None:\n",
        "                    page_uri = record.rec_headers.get_header('WARC-Target-URI')\n",
        "                    clean_text = get_page_content(html_content)\n",
        "                    try:\n",
        "                        if detect(clean_text) == 'en':\n",
        "                            yield clean_text, page_uri\n",
        "                    except langdetect.lang_detect_exception.LangDetectException:\n",
        "                        # traceback.print_exc()\n",
        "                        continue"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0qXt9Ts9S6a"
      },
      "source": [
        "relevent_uri = list()\n",
        "\n",
        "warc_list = [\n",
        "    'https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-50/warc.paths.gz',\n",
        "    'https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-45/warc.paths.gz',\n",
        "    'https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-40/warc.paths.gz',\n",
        "    'https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-34/warc.paths.gz',\n",
        "    'https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-29/warc.paths.gz',\n",
        "    'https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-24/warc.paths.gz',\n",
        "    'https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2020-16/warc.paths.gz'\n",
        "]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jZzJIan9Uq5"
      },
      "source": [
        "for warc_all_uri in warc_list:\n",
        "    try:\n",
        "        web_response = requests.get(warc_all_uri, stream=True)\n",
        "        gz_file = web_response.content\n",
        "\n",
        "        f = io.BytesIO(gz_file)\n",
        "        with gzip.GzipFile(fileobj=f) as fh:\n",
        "            for incomplete_uri in fh:\n",
        "                incomplete_uri = incomplete_uri.decode().replace('\\n', '')\n",
        "                warc_uri = f'https://commoncrawl.s3.amazonaws.com/{incomplete_uri}'\n",
        "                # print(f'Extracting from: {warc_uri}')\n",
        "                for doc_str, page_uri in get_content_from_warc(warc_uri):\n",
        "                    doc_vector = vectorizer.transform([doc_str])\n",
        "                    cos_sim = np.average(cosine_similarity(doc_vector, docs_tfidf).flatten())\n",
        "                    if cos_sim > 0.2:\n",
        "                        print(page_uri)\n",
        "                        relevent_uri.append(page_uri)\n",
        "                \n",
        "                if len(relevent_uri) > 1000:\n",
        "                    break\n",
        "    except:\n",
        "        with open('relevant_uri_2.pkl', 'wb') as f:\n",
        "            pickle.dump(relevent_uri, f)\n",
        "            traceback.print_exc()\n",
        "        sys.exit(0)\n",
        "\n",
        "with open('relevant_uri_2.pkl', 'wb') as f:\n",
        "    pickle.dump(relevent_uri, f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}